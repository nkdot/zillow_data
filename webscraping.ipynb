{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import urllib.request \n",
    "import urllib.parse\n",
    "import urllib.error\n",
    "import ssl\n",
    "from urllib.request import Request, urlopen\n",
    "from bs4 import BeautifulSoup as BS\n",
    "import time\n",
    "\n",
    "# For ignoring SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#price\n",
    "def get_general_data():\n",
    "    l_price=[]\n",
    "    bed=[]\n",
    "    baths=[]\n",
    "    status_type=[]\n",
    "    status_text=[]\n",
    "    zipcode=[]\n",
    "    house_link=[]\n",
    "    address=[]\n",
    "    city=[]\n",
    "    sqrft=[]\n",
    "    price_sqft=[]\n",
    "    price_change=[]\n",
    "    buildername=[]\n",
    "    price_reduction=[]\n",
    "    data=result['cat1']['searchResults']['listResults'] #the necessary data is in a nested dictionary\n",
    "    for house in data:\n",
    "        l_price.append(house['price'])\n",
    "        bed.append(house['beds'])\n",
    "        baths.append(house['baths'])\n",
    "        status_type.append(house['statusType'])\n",
    "        status_text.append(house['statusText'])\n",
    "        zipcode.append(house['addressZipcode'])\n",
    "        house_link.append(house['detailUrl'])\n",
    "        address.append(house['address'])\n",
    "        city.append(house['hdpData']['homeInfo']['city'])\n",
    "        sqrft.append(house['area'])\n",
    "    df['List Price']=l_price\n",
    "    df['No of Beds']=bed\n",
    "    df['No of bath']=baths\n",
    "    df['Status_type']=status_type\n",
    "    df['Status Text']=status_text\n",
    "    df['Zipcode']=zipcode\n",
    "    df['Address']=address\n",
    "    df['City']=city\n",
    "    df['Sqrft']=sqrft\n",
    "    df['url']=house_link\n",
    "    \n",
    "    print('list price',len(l_price))\n",
    "    print('# bed',len(bed))\n",
    "    print('#bath',len(baths))\n",
    "    print('Status_type',len(status_type))\n",
    "    print('Status Text',len(status_text))\n",
    "    print('Zipcode',len(zipcode))\n",
    "    print('address',len(address))\n",
    "    print('city',len(city))\n",
    "    print('sqrft',len(sqrft))\n",
    "    print('url',len(house_link))\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get Price/sqft for each house\n",
    "\n",
    "def get_house_specific_data():\n",
    "    import time\n",
    "    time.sleep(0.5)\n",
    "    house_types=[]\n",
    "    year_built=[]\n",
    "    price_per_sf=[]\n",
    "    HOA=[]\n",
    "    E_rating=[]\n",
    "    J_rating=[]\n",
    "    H_rating=[]\n",
    "    time_zillow=[]\n",
    "    views=[]\n",
    "    saves=[]\n",
    "\n",
    "    urls=df['url']\n",
    "\n",
    "    for j in urls:\n",
    "        \n",
    "        req1 = Request(j, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        webpage1 = urlopen(req1)\n",
    "        soup1=BS(webpage1,'html.parser')\n",
    "\n",
    "        home_facts=soup1.findAll('li',attrs={'class':'ds-home-fact-list-item'})\n",
    "\n",
    "        house_type=re.findall('^.*:(.*)$',home_facts[0].get_text())[0]\n",
    "        yr_built=re.findall('^.*:(.*)$',home_facts[1].get_text())[0]\n",
    "        \n",
    "        home_details=''\n",
    "        for i in home_facts:\n",
    "            home_details+=i.get_text()\n",
    "            \n",
    "        try:\n",
    "            ps=re.findall('^.*Price/sqft:(\\$[0-9,]{1,6}).*$',home_details)[0]\n",
    "            price_per_sf.append(ps)\n",
    "        except:\n",
    "            price_per_sf.append('NA')\n",
    "        \n",
    "        try:\n",
    "            hoa=re.findall('^.*HOA:(\\$[0-9,]{1,5}).*$',home_details)[0]\n",
    "            HOA.append(hoa)\n",
    "        except:\n",
    "            HOA.append('NA')\n",
    "        \n",
    "        house_types.append(house_type)\n",
    "        year_built.append(yr_built)    \n",
    "        \n",
    "        #school rating\n",
    "        content=soup1.findAll('div',attrs={'class':'Spacer-c11n-8-48-0__sc-17suqs2-0 bcAgQH'})\n",
    "        str1=''\n",
    "        for i in content:\n",
    "            str1+=i.get_text()\n",
    "        rating=re.findall('([0-9]{1,2})/10',str1)\n",
    "        #print(rating)\n",
    "        try:\n",
    "            if len(rating)==3:\n",
    "                E_rating.append(rating[0])\n",
    "                J_rating.append(rating[1])\n",
    "                H_rating.append(rating[2])\n",
    "            elif len(rating)==2:\n",
    "                if len(re.findall('(Elementary|K-5|K-6)',str1))==0:\n",
    "                    E_rating.append('NA')\n",
    "                    J_rating.append(rating[0])\n",
    "                    H_rating.append(rating[1])\n",
    "                elif len(re.findall('(Middle|Junior High|Junior|7-8)',str1))==0:\n",
    "                    E_rating.append(rating[0])\n",
    "                    H_rating.append(rating[1])\n",
    "                    J_rating.append('NA')\n",
    "                elif len(re.findall('(Junior High)',str1))==0 and len(re.findall('(High|9-12)',str1))==1:\n",
    "                    E_rating.append(rating[0])\n",
    "                    J_rating.append(rating[1])\n",
    "                    H_rating.append('NA')\n",
    "                else:\n",
    "                    E_rating.append('NA')\n",
    "                    J_rating.append('NA')\n",
    "                    H_rating.append('NA')\n",
    "                    \n",
    "            elif len(rating)==1:\n",
    "                if len(re.findall('(Elementary|K-5|K-6)',str1))==1:\n",
    "                    E_rating.append(rating[0])\n",
    "                    J_rating.append('NA')\n",
    "                    H_rating.append('NA')\n",
    "                elif len(re.findall('(Middle|Junior High||Junior|7-8)',str1))==1:\n",
    "                    J_rating.append(rating[0])\n",
    "                    E_rating.append('NA')\n",
    "                    H_rating.append('NA')\n",
    "                elif len(re.findall('(High|9-12)',str1))==1:\n",
    "                    H_rating.append(rating[0])\n",
    "                    E_rating.append('NA')\n",
    "                    J_rating.append('NA')\n",
    "                else:\n",
    "                    E_rating.append('NA')\n",
    "                    J_rating.append('NA')\n",
    "                    H_rating.append('NA')\n",
    "                        \n",
    "            else:\n",
    "                E_rating.append('NA')\n",
    "                J_rating.append('NA')\n",
    "                H_rating.append('NA')\n",
    "        except:\n",
    "            E_rating.append('NA')\n",
    "            J_rating.append('NA')\n",
    "            H_rating.append('NA')\n",
    "        \n",
    "            \n",
    "        #time,saves & views\n",
    "        \n",
    "        \n",
    "        count=soup1.findAll('div',attrs={'class':\"ds-expandable-card-section-default-padding\"})\n",
    "        tsv=''\n",
    "        for i in count:\n",
    "            tsv+=i.get_text()\n",
    "        try:\n",
    "            time=re.findall('Time on Zillow([0-9,]+) days',tsv)[0]\n",
    "            time_zillow.append(time)\n",
    "        except:\n",
    "            time_zillow.append('NA')\n",
    "        try:\n",
    "            view=re.findall('Views([0-9,]+)',tsv)[0]\n",
    "            views.append(view)\n",
    "        except:\n",
    "            views.append('NA')\n",
    "        try:\n",
    "            save=re.findall('Saves([0-9,]+)',tsv)[0]\n",
    "            saves.append(save)\n",
    "        except:\n",
    "            saves.append('NA')\n",
    "        \n",
    "\n",
    "        \n",
    "    df['House_type']=house_types\n",
    "    df['Year Built']=year_built\n",
    "    df['Price/sqft']=price_per_sf\n",
    "    df['Time on Zillow']=time_zillow\n",
    "    df['Views']=views\n",
    "    df['Saves']=saves\n",
    "    df[ 'Elementary school rating']=E_rating\n",
    "    df['Junior school rating']=J_rating\n",
    "    df['High school rating']=H_rating\n",
    "    df['HOA Fee']=HOA\n",
    "        \n",
    "    print('House_type:',len(house_types))\n",
    "    print('Year Built',len(year_built))\n",
    "    print('price/sqrtft',len(price_per_sf))\n",
    "    print('HOA Fee',len(HOA))\n",
    "        \n",
    "    print('E_rating length:',len(E_rating))\n",
    "    print('J_rating length:',len(J_rating))\n",
    "    print('H_rating length:',len(H_rating))\n",
    "    print('saves length',len(saves))\n",
    "    print('views lenth',len(views))\n",
    "    print('Time on zillow length',len(time_zillow))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_dataframe():\n",
    "    data=pd.DataFrame(df)\n",
    "    writer=pd.ExcelWriter('zillow_page{}.xlsx'.format(i))\n",
    "    data.to_excel(writer)\n",
    "    writer.save()\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "url: https://www.zillow.com/san-francisco-ca/\n",
    "(pages 1-20)\n",
    "\n",
    "https://www.zillow.com/fremont-ca/\n",
    "(5 pages)\n",
    "\n",
    "https://www.zillow.com/fremont-ca/sold/\n",
    "(20 pages)\n",
    "\n",
    "https://www.zillow.com/oakland-ca/\n",
    "(20 pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to run?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Enter the following input to scrape pages from 1 to 3 in fremont\n",
    "link = https://www.zillow.com/fremont-ca/\n",
    "start page= 1\n",
    "End page=3\n",
    "\n",
    "We will get the scraped output xlsx files for 3 pages in zillow_page1,zillow_page2 & zillow_page3  xlsx files respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the url:https://www.zillow.com/fremont-ca/\n",
      "start page1\n",
      "End page2\n",
      "https://www.zillow.com/fremont-ca/1_p/\n",
      "list price 40\n",
      "# bed 40\n",
      "#bath 40\n",
      "Status_type 40\n",
      "Status Text 40\n",
      "Zipcode 40\n",
      "address 40\n",
      "city 40\n",
      "sqrft 40\n",
      "url 40\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-a61dbcc9cb8e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mdf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mget_general_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[0mget_house_specific_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[0mto_dataframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-68d497bde1f5>\u001b[0m in \u001b[0;36mget_house_specific_data\u001b[1;34m()\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mhome_facts\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msoup1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindAll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'li'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'class'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m'ds-home-fact-list-item'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[0mhouse_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'^.*:(.*)$'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhome_facts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[0myr_built\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'^.*:(.*)$'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhome_facts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "link=input('Enter the url:')\n",
    "sp=int(input('start page'))\n",
    "ep=int(input('End page'))\n",
    "for i in range(sp,ep+1):\n",
    "    time.sleep(2)\n",
    "    url=str(link)+str(i)+'_p/'\n",
    "    print(url)\n",
    "    req=Request(url, headers={'User-Agent':'Mozilla/5.0'})\n",
    "    webpage=urlopen(req).read()\n",
    "    soup=BS(webpage, 'html.parser')\n",
    "    house_info=soup.find(text=re.compile('<!--(.+?)-->')) #locate data from comments; there are more comments but they aren't important, so I use .find, not .findAll\n",
    "    house_info\n",
    "    house_info=house_info[4:-3] #get rid of the comment indicators\n",
    "    result=json.loads(house_info)\n",
    "    result\n",
    "    \n",
    "    df=dict()\n",
    "    get_general_data()\n",
    "    get_house_specific_data()\n",
    "    to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
